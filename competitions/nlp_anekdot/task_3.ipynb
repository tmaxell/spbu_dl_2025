{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U \"transformers>=4.51.0\" \"datasets>=2.18.0\" \"accelerate>=0.33.0\" \"peft>=0.12.0\" \"bitsandbytes>=0.43.0\" \"safetensors>=0.4.3\" \"tqdm>=4.66.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, random, time\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling, set_seed\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cuda:\", torch.cuda.is_available())\n",
    "assert torch.cuda.is_available(), \"GPU не включен: Runtime -> Change runtime type -> GPU\"\n",
    "print(\"gpu:\", torch.cuda.get_device_name(0))\n",
    "!nvidia-smi\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9dc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_path = \"/content/prefixes.txt\"\n",
    "assert os.path.exists(prefix_path), f\"Нет файла {prefix_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_prefixes(path):\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip(\"\\n\").strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            m = re.match(r\"^\\s*(\\d+)\\s+(.*)\\s*$\", s)\n",
    "            if m:\n",
    "                idx = int(m.group(1))\n",
    "                pref = m.group(2).strip()\n",
    "                items.append((idx, pref))\n",
    "            else:\n",
    "                items.append((len(items), s))\n",
    "    items.sort(key=lambda x: x[0])\n",
    "    return items\n",
    "prefixes = parse_prefixes(prefix_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff75f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prefix_regex(prefix):\n",
    "    p = prefix.strip()\n",
    "    p = p.replace(\"—\", \"-\").replace(\"–\", \"-\")\n",
    "    esc = re.escape(p)\n",
    "    esc = esc.replace(r\"\\ \", r\"\\s+\")\n",
    "    esc = esc.replace(\"ё\", \"[её]\").replace(\"Ё\", \"[ЕЁ]\")\n",
    "    esc = esc.replace(r\"\\-\", r\"[-—–]\")\n",
    "    return re.compile(r\"^\\s*\" + esc + r\"(?:(?:\\s*[,:\\-—–]\\s*)|\\s+|$)\", re.IGNORECASE)\n",
    "patterns = {idx: build_prefix_regex(pref) for idx, pref in prefixes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_one_line(s):\n",
    "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    s = s.strip(\" ,;:-—–\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "def safe_continuation(full, prefix):\n",
    "    full_s = full.strip()\n",
    "    pref_s = prefix.strip()\n",
    "    if full_s.lower().startswith(pref_s.lower()):\n",
    "        cont = full_s[len(pref_s):]\n",
    "    else:\n",
    "        pos = full_s.lower().find(pref_s.lower())\n",
    "        cont = full_s[pos + len(pref_s):] if pos >= 0 else full_s\n",
    "    return cleanup_one_line(cont)\n",
    "def score_candidate(s):\n",
    "    if not s:\n",
    "        return -1e9\n",
    "    if len(s) < 25:\n",
    "        return -200 + len(s)\n",
    "    if len(s) > 280:\n",
    "        return -50 - (len(s) - 280)\n",
    "    words = re.findall(r\"[A-Za-zА-Яа-яЁё0-9]+\", s)\n",
    "    uniq = len(set(w.lower() for w in words)) / max(1, len(words))\n",
    "    rep_pen = 0\n",
    "    if len(words) >= 10:\n",
    "        for i in range(len(words) - 6):\n",
    "            if words[i:i+3] == words[i+3:i+6]:\n",
    "                rep_pen += 1\n",
    "    end_bonus = 10 if re.search(r\"[.!?…]$\", s) else 0\n",
    "    return 60 * uniq + end_bonus - 25 * rep_pen - 0.03 * abs(len(s) - 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_prefix_real_limit = 60\n",
    "general_limit = 6000\n",
    "max_scan = 90000\n",
    "counts = {idx: 0 for idx, _ in prefixes}\n",
    "active = set(counts.keys())\n",
    "real_texts = []\n",
    "general_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stage: streaming/filtering dataset\")\n",
    "ds_stream = load_dataset(\"igorktech/anekdots\", split=\"train\", streaming=True)\n",
    "pbar = tqdm(total=max_scan, desc=\"scan\", unit=\"ex\")\n",
    "scanned = 0\n",
    "for ex in ds_stream:\n",
    "    scanned += 1\n",
    "    pbar.update(1)\n",
    "    if scanned >= max_scan:\n",
    "        break\n",
    "    t = ex.get(\"text\", None)\n",
    "    if not isinstance(t, str):\n",
    "        continue\n",
    "    t = t.strip()\n",
    "    if len(t) < 60 or len(t) > 900:\n",
    "        continue\n",
    "    if \"\\u0000\" in t:\n",
    "        continue\n",
    "    mark = ex.get(\"total_mark\", None)\n",
    "    if isinstance(mark, int) and mark < 3:\n",
    "        continue\n",
    "    matched = False\n",
    "    if active:\n",
    "        for idx in list(active):\n",
    "            if counts[idx] >= per_prefix_real_limit:\n",
    "                active.discard(idx)\n",
    "                continue\n",
    "            if patterns[idx].match(t):\n",
    "                real_texts.append(t)\n",
    "                counts[idx] += 1\n",
    "                matched = True\n",
    "                if counts[idx] >= per_prefix_real_limit:\n",
    "                    active.discard(idx)\n",
    "                break\n",
    "    if (not matched) and (len(general_texts) < general_limit):\n",
    "        if isinstance(mark, int) and mark >= 10:\n",
    "            general_texts.append(t)\n",
    "    if scanned % 2000 == 0:\n",
    "        pbar.set_postfix({\"real\": len(real_texts), \"gen\": len(general_texts), \"active\": len(active)})\n",
    "    if (len(general_texts) >= general_limit) and (not active):\n",
    "        break\n",
    "pbar.close()\n",
    "print(\"Stage done:\", {\"scanned\": scanned, \"real\": len(real_texts), \"gen\": len(general_texts), \"active\": len(active)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = real_texts + general_texts\n",
    "random.shuffle(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d204318",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_dir = \"/content/qwen3_0p6b_ru_jokes_lora\"\n",
    "os.makedirs(adapter_dir, exist_ok=True)\n",
    "base_model_id = \"Qwen/Qwen3-0.6B-Base\"\n",
    "print(\"Model:\", base_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ee355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base():\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=compute_dtype,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "def generate_candidates(m, tok, prefix, n, max_new_tokens=90, temperature=1.0, top_p=0.9, rep_pen=1.12):\n",
    "    inp = tok(prefix, return_tensors=\"pt\")\n",
    "    input_ids = inp[\"input_ids\"].to(m.device)\n",
    "    attn = inp[\"attention_mask\"].to(m.device)\n",
    "    input_ids = input_ids.repeat(n, 1)\n",
    "    attn = attn.repeat(n, 1)\n",
    "    with torch.no_grad():\n",
    "        out = m.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attn,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=rep_pen,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "            eos_token_id=tok.eos_token_id\n",
    "        )\n",
    "    return [tok.decode(out[i], skip_special_tokens=True) for i in range(out.size(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_exists = os.path.exists(os.path.join(adapter_dir, \"adapter_config.json\"))\n",
    "if not adapter_exists:\n",
    "    model_for_aug = load_base()\n",
    "    model_for_aug.eval()\n",
    "    aug_per_prefix = 1\n",
    "    aug_texts = []\n",
    "    for _, pref in tqdm(prefixes, desc=\"augment\", unit=\"pref\"):\n",
    "        fulls = generate_candidates(model_for_aug, tokenizer, pref, n=aug_per_prefix, max_new_tokens=80, temperature=1.02, top_p=0.9, rep_pen=1.10)\n",
    "        for ft in fulls:\n",
    "            ft = cleanup_one_line(ft)\n",
    "            if len(ft) >= len(pref) + 20:\n",
    "                aug_texts.append(ft)\n",
    "    train_texts = train_texts + aug_texts\n",
    "    random.shuffle(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not adapter_exists:\n",
    "    model = load_base()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.config.use_cache = False\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not adapter_exists:\n",
    "    train_ds = Dataset.from_dict({\"text\": train_texts})\n",
    "    max_len = 192\n",
    "    def tok_fn(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, max_length=max_len, padding=False)\n",
    "    tok_ds = train_ds.map(tok_fn, batched=True, remove_columns=[\"text\"], desc=\"tokenize\")\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not adapter_exists:\n",
    "    class TrainTimeCallback(TrainerCallback):\n",
    "        def __init__(self):\n",
    "            self.t0 = None\n",
    "        def on_train_begin(self, args, state, control, **kwargs):\n",
    "            self.t0 = time.time()\n",
    "            print(f\"train_begin: max_steps={state.max_steps} epochs={args.num_train_epochs} bs={args.per_device_train_batch_size} ga={args.gradient_accumulation_steps} bf16={args.bf16} fp16={args.fp16}\")\n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            if logs is None:\n",
    "                return\n",
    "            now = time.time()\n",
    "            elapsed = now - (self.t0 or now)\n",
    "            step = state.global_step\n",
    "            max_steps = state.max_steps or 0\n",
    "            eta = None\n",
    "            if max_steps and step:\n",
    "                eta = elapsed * (max_steps - step) / max(1, step)\n",
    "            loss = logs.get(\"loss\", None)\n",
    "            lr = logs.get(\"learning_rate\", None)\n",
    "            ep = state.epoch\n",
    "            s = f\"step={step}/{max_steps} epoch={ep:.3f}\" if ep is not None else f\"step={step}/{max_steps}\"\n",
    "            if loss is not None:\n",
    "                s += f\" loss={loss:.4f}\"\n",
    "            if lr is not None:\n",
    "                s += f\" lr={lr:.2e}\"\n",
    "            s += f\" elapsed={elapsed/60:.1f}m\"\n",
    "            if eta is not None:\n",
    "                s += f\" eta={eta/60:.1f}m\"\n",
    "            print(s)\n",
    "        def on_train_end(self, args, state, control, **kwargs):\n",
    "            if self.t0 is None:\n",
    "                return\n",
    "            elapsed = time.time() - self.t0\n",
    "            print(f\"train_end: steps={state.global_step} time={elapsed/60:.1f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3377b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not adapter_exists:\n",
    "    args = TrainingArguments(\n",
    "        output_dir=adapter_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        bf16=use_bf16,\n",
    "        fp16=not use_bf16,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=2,\n",
    "        dataloader_pin_memory=True,\n",
    "        group_by_length=True,\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tok_ds,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[TrainTimeCallback()]\n",
    "    )\n",
    "    print(\"Stage: training\")\n",
    "    trainer.train()\n",
    "    model.save_pretrained(adapter_dir)\n",
    "    tokenizer.save_pretrained(adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ca44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = load_base()\n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8608849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(prefix, n, max_new_tokens=110, temperature=1.03, top_p=0.9, rep_pen=1.12):\n",
    "    inp = tokenizer(prefix, return_tensors=\"pt\")\n",
    "    input_ids = inp[\"input_ids\"].to(model.device)\n",
    "    attn = inp[\"attention_mask\"].to(model.device)\n",
    "    input_ids = input_ids.repeat(n, 1)\n",
    "    attn = attn.repeat(n, 1)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attn,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=rep_pen,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return [tokenizer.decode(out[i], skip_special_tokens=True) for i in range(out.size(0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b60b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_path = \"/content/submission.txt\"\n",
    "num_lines_per_prefix = 3\n",
    "candidates_per_prefix = 12\n",
    "print(\"Stage: generating submission\")\n",
    "with open(submission_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, pref in tqdm(prefixes, desc=\"prefixes\", unit=\"pref\"):\n",
    "        fulls = generate_batch(pref, candidates_per_prefix)\n",
    "        cands = []\n",
    "        for ft in fulls:\n",
    "            cont = safe_continuation(ft, pref)\n",
    "            if cont:\n",
    "                cands.append(cont)\n",
    "        cands = list(dict.fromkeys(cands))\n",
    "        cands.sort(key=score_candidate, reverse=True)\n",
    "        picked = cands[:num_lines_per_prefix] if cands else [\"...\"]\n",
    "        for cont in picked:\n",
    "            f.write(f\"{idx} {cont}\\n\")\n",
    "print(\"saved:\", submission_path)\n",
    "with open(submission_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(15):\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        print(line.rstrip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6845a80",
   "metadata": {},
   "source": [
    "вырезал с префиксом 0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "state": {},
   "version_major": 2,
   "version_minor": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
