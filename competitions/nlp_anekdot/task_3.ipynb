{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10bb490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B-Base\"\n",
    "DATASET_NAME = \"igorktech/anekdots\"\n",
    "OUT_DIR = \"qwen3_06b_anekdots_lora\"\n",
    "MAX_LEN = 384\n",
    "TRAIN_SIZE = 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7050d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0ebce44ca94c5e9da7df13d7d26765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be37758b878474ea8106772470214a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf2c9dacd5f4b4e8d743b5b90c192ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59106f9d4ff40408133442a97416427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_compute_dtype():\n",
    "    if not torch.cuda.is_available():\n",
    "        return torch.float32\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "    if major >= 8:\n",
    "        return torch.bfloat16\n",
    "    return torch.float16\n",
    "\n",
    "compute_dtype = get_compute_dtype()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc6aecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e0c8dad5d34ff98f6e78c1e00183b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4921179e57462d9c097613725c49ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m base_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     MODEL_NAME,\n\u001b[1;32m      3\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,\n\u001b[1;32m      5\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m base_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m base_model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(base_model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    605\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    606\u001b[0m     )\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:5048\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5039\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   5041\u001b[0m     (\n\u001b[1;32m   5042\u001b[0m         model,\n\u001b[1;32m   5043\u001b[0m         missing_keys,\n\u001b[1;32m   5044\u001b[0m         unexpected_keys,\n\u001b[1;32m   5045\u001b[0m         mismatched_keys,\n\u001b[1;32m   5046\u001b[0m         offload_index,\n\u001b[1;32m   5047\u001b[0m         error_msgs,\n\u001b[0;32m-> 5048\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   5049\u001b[0m         model,\n\u001b[1;32m   5050\u001b[0m         state_dict,\n\u001b[1;32m   5051\u001b[0m         checkpoint_files,\n\u001b[1;32m   5052\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   5053\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   5054\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   5055\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   5056\u001b[0m         disk_offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   5057\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   5058\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   5059\u001b[0m         keep_in_fp32_regex\u001b[38;5;241m=\u001b[39mkeep_in_fp32_regex,\n\u001b[1;32m   5060\u001b[0m         device_mesh\u001b[38;5;241m=\u001b[39mdevice_mesh,\n\u001b[1;32m   5061\u001b[0m         key_mapping\u001b[38;5;241m=\u001b[39mkey_mapping,\n\u001b[1;32m   5062\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[1;32m   5063\u001b[0m     )\n\u001b[1;32m   5064\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   5065\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:5468\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5465\u001b[0m         args_list \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mtqdm(args_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[0;32m-> 5468\u001b[0m         _error_msgs, disk_offload_index \u001b[38;5;241m=\u001b[39m load_shard_file(args)\n\u001b[1;32m   5469\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _error_msgs\n\u001b[1;32m   5471\u001b[0m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:831\u001b[0m, in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# If shard_file is \"\", we use the existing state_dict instead of loading it\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shard_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 831\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m load_state_dict(\n\u001b[1;32m    832\u001b[0m         shard_file, is_quantized\u001b[38;5;241m=\u001b[39mis_quantized, map_location\u001b[38;5;241m=\u001b[39mmap_location, weights_only\u001b[38;5;241m=\u001b[39mweights_only\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Fix the key names\u001b[39;00m\n\u001b[1;32m    836\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m {key_renaming_mapping[k]: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m key_renaming_mapping}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:495\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 495\u001b[0m         _slice \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget_slice(k)\n\u001b[1;32m    496\u001b[0m         k_dtype \u001b[38;5;241m=\u001b[39m _slice\u001b[38;5;241m.\u001b[39mget_dtype()\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k_dtype \u001b[38;5;129;01min\u001b[39;00m str_to_torch_dtype:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    s = s.replace(\"\\r\", \"\\n\")\n",
    "    s = re.sub(r\"\\n{2,}\", \"\\n\", s)\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def build_prompt(prefix):\n",
    "    return f\"Продолжи анекдот на русском языке.\\nЗатравка: {prefix}\\nАнекдот:\"\n",
    "\n",
    "def find_sublist(haystack, needle):\n",
    "    if len(needle) == 0:\n",
    "        return -1\n",
    "    for i in range(0, len(haystack) - len(needle) + 1):\n",
    "        if haystack[i : i + len(needle)] == needle:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "response_marker = \"Анекдот:\"\n",
    "response_ids = tokenizer(response_marker, add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(DATASET_NAME, split=\"train\")\n",
    "ds = ds.shuffle(seed=42).select(range(min(TRAIN_SIZE, len(ds))))\n",
    "\n",
    "def make_pair(example, idx):\n",
    "    text = normalize_text(example[\"text\"])\n",
    "    if len(text) < 40:\n",
    "        return {\"text\": None}\n",
    "    ids = tokenizer(text, add_special_tokens=False).input_ids\n",
    "    if len(ids) < 60:\n",
    "        return {\"text\": None}\n",
    "    rng = random.Random(1000003 + idx)\n",
    "    max_cut = min(32, len(ids) - 24)\n",
    "    if max_cut <= 8:\n",
    "        return {\"text\": None}\n",
    "    cut = rng.randint(8, max_cut)\n",
    "    prefix = tokenizer.decode(ids[:cut], skip_special_tokens=True).strip()\n",
    "    cont = tokenizer.decode(ids[cut:], skip_special_tokens=True).strip()\n",
    "    if len(prefix) < 8 or len(cont) < 20:\n",
    "        return {\"text\": None}\n",
    "    prompt = build_prompt(prefix)\n",
    "    full = prompt + \" \" + cont + tokenizer.eos_token\n",
    "    return {\"text\": full}\n",
    "\n",
    "ds_pairs = ds.map(make_pair, with_indices=True, remove_columns=ds.column_names, num_proc=os.cpu_count() or 2)\n",
    "ds_pairs = ds_pairs.filter(lambda x: x[\"text\"] is not None)\n",
    "ds_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18fa8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN, add_special_tokens=False)\n",
    "\n",
    "tok_ds = ds_pairs.map(tokenize_fn, batched=True, remove_columns=[\"text\"], num_proc=os.cpu_count() or 2)\n",
    "tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074eac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionCollator:\n",
    "    def __init__(self, tokenizer, response_ids):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.response_ids = response_ids\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch = self.tokenizer.pad(features, return_tensors=\"pt\")\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        for i in range(labels.size(0)):\n",
    "            ids = labels[i].tolist()\n",
    "            start = find_sublist(ids, self.response_ids)\n",
    "            if start == -1:\n",
    "                labels[i, :] = -100\n",
    "            else:\n",
    "                end = start + len(self.response_ids)\n",
    "                labels[i, :end] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "collator = CompletionCollator(tokenizer, response_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e875b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16 = torch.cuda.is_available() and compute_dtype == torch.bfloat16\n",
    "fp16 = torch.cuda.is_available() and compute_dtype == torch.float16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    bf16=bf16,\n",
    "    fp16=fp16,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tok_ds,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(OUT_DIR)\n",
    "tokenizer.save_pretrained(OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba31ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prefixes(path=\"prefixes.txt\"):\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if not s:\n",
    "                continue\n",
    "            m = re.match(r\"^(\\d+)\\s+(.*)$\", s)\n",
    "            if m:\n",
    "                idx = int(m.group(1))\n",
    "                pref = m.group(2).strip()\n",
    "                items.append((idx, pref))\n",
    "            else:\n",
    "                items.append((len(items), s))\n",
    "    return items\n",
    "\n",
    "prefixes = read_prefixes(\"prefixes.txt\")\n",
    "len(prefixes), prefixes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a92110",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, OUT_DIR)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUT_DIR, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_substrings = [\n",
    "    \"Затравка:\",\n",
    "    \"Анекдот:\",\n",
    "    \"<think>\",\n",
    "    \"</think>\",\n",
    "]\n",
    "\n",
    "def clean_continuation(s, prefix):\n",
    "    s = s.replace(\"\\r\", \"\\n\")\n",
    "    for b in bad_substrings:\n",
    "        s = s.replace(b, \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    if s.startswith(prefix):\n",
    "        s = s[len(prefix):].lstrip(\" .,!?:;—-\")\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def generate_continuations(prefix, n=5, max_new_tokens=96, temperature=0.9, top_p=0.92, top_k=50, repetition_penalty=1.08):\n",
    "    prompt = build_prompt(prefix)\n",
    "    prompts = [prompt] * n\n",
    "    enc = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    input_lens = enc[\"attention_mask\"].sum(dim=1).tolist()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            no_repeat_ngram_size=3,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    results = []\n",
    "    for i in range(out.size(0)):\n",
    "        gen_ids = out[i, input_lens[i]:]\n",
    "        txt = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "        txt = clean_continuation(txt, prefix)\n",
    "        txt = txt.replace(\"\\n\", \" \").strip()\n",
    "        txt = re.sub(r\"\\s+\", \" \", txt).strip()\n",
    "        if len(txt) >= 8:\n",
    "            results.append(txt)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63000945",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PER_PREFIX = 5\n",
    "all_lines = []\n",
    "\n",
    "for idx, pref in prefixes:\n",
    "    conts = generate_continuations(pref, n=N_PER_PREFIX)\n",
    "    if len(conts) == 0:\n",
    "        conts = [\"...\"]\n",
    "    for c in conts:\n",
    "        all_lines.append(f\"{idx} {c}\")\n",
    "\n",
    "out_path = \"anekdots_submission.txt\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in all_lines:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "out_path, len(all_lines), all_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cd8bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"anekdots_submission.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(25):\n",
    "        print(f.readline().rstrip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
