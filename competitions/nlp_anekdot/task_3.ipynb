{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FVjc1hmv4Iwr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "619db456f00f4029a0e14aef45d9a3ea",
            "3761eac90c694f5abadec59fa620cc35",
            "263e01f35c764e75ad2b115fbd38f7bf",
            "2c20adee9435401f855e21718fc5651f",
            "e92bf2f4be78421c8fb5b7b4cc90cb33",
            "89833ad6969b4bbd883821af663c8c4d",
            "63117e8a0a634af9b0bc6b64b37c25f0",
            "875f92d4bb5a4f93bd7f63d6d70c1974",
            "779999ed60d34aff83d553a376a457f8",
            "469ae0d329aa4f44bc92068a34f29082",
            "8d28f763aa9843bab6c9d2f9d73f20e3",
            "1e8066a20f084629a807e40e893865c0",
            "f647c81b22364d94908a457456090b59",
            "3687c96b0c6c44be95c24390d1536b89",
            "44695a4107984e5992d9943dd3908f46",
            "54b19b68e25849a8be72f7fa680380ba",
            "389890fc135c476dbdb74f87d30e9b91",
            "2c344bab1db847ddbe7ae98d17d7e91b",
            "d3356b13723c4ba1986380ec15ddeee7",
            "7d548fb1b29d43c486f8153bba774960",
            "e51343cab248419c9d1e182127d9385b",
            "bdd2713c53104780a078e9adf33053de",
            "a93063a762dd4e2cb53654cabba22d45",
            "307594ff832f42dd881f496e1be518b1",
            "c77c291c1a35412e961a7a674b2fd31a",
            "9de7a83f82f34e379f530b053dda02ba",
            "4d015239710a4b1ba0d6638725c31c0d",
            "5aa58180c7334498936de66ed6ca0b75",
            "12e59dd5fc354becb362f3316f2b879b",
            "1d07c9810f84481c9f9a88776f248b29",
            "957f8efe9a7b4123a68b497475985d50",
            "9d9717a3b2a249cab28f508eaed50cf7",
            "a0e2f6f659744e26805f428477b32ce2",
            "5f6937db84ce4af682853f137ed482a1",
            "e569063351454c468a65c03990250316",
            "f1703103ed7d46d0b83251d4da88acbd",
            "d6481d85ed2844b587f9338de8ac6b83",
            "c46c6d68d88348358d10d255282e9d76",
            "c9d065d8187b44afa3383b9f02008d59",
            "236c61aed0da454d8a2800e49eedf987",
            "31b49b6f044249dbb7874db61ee8c5fe",
            "b638bd02396f4e5d92a804a46a9285ec",
            "74f9cc7d6d1141f6a56fad32447c0962",
            "c495743dc4f44f97a9843ec986e36884",
            "5cdd556ad73b4333b9711e75a5931daf",
            "05fe22aa7ced40379521e9f0312379a8",
            "b1222d1bb11f44f389cc1600dd88bbae",
            "2ce1b7ce3133425794e88a4912117ea3",
            "7eb66ed2e2dd4d9db74d1cd1aa5b12db",
            "03c203898ad448b08694112647642535",
            "cd41ce1528f74070a68ab67b96c6df0a",
            "4110d6d96a764a97a95f41a6c7035a58",
            "9bd042a5e5b1405a80fab9194755c843",
            "39a7ffc95fc9408eb6ba2f776853c6e2",
            "a8238fc4f0494bb997c2d25cdbb4e53c",
            "249057f3158845c487b837671c6eb7b2",
            "6191acf2ad9c48f4b274404c5cb2a4e1",
            "4ad8e2fa861e4005b62fa527de6565b3",
            "16ac2d9d86e54643aba37e89b00dc9b7",
            "a0c827163b444e3f89599c822ece73fd",
            "419d7ec42ea6400382498b1e1548ad76",
            "b77f581fbcfc4ad181851833a6a9e99a",
            "fce19ab03c774bc6a63fdf5b91b773e6",
            "f57bc888e6fe4c0d8bfa502ebc8d3493",
            "e5952121c7f84846a5eb1a475435e463",
            "331f59a9d27543ba8890785a84d06ea2",
            "53ec627bac9f4a689b70b5d9321a9bf0",
            "9a4ece939951411c931d1d367425d6cf",
            "397f074e56ce4131bcbbb2bf40dc84fe",
            "0cab28d8213942b98319a617de6b6dfd",
            "45bd007222c447cdb184373dfb6de348",
            "958e70114b4c4fbd93cc377062c5d53c",
            "821cefcb6ebd4fe9bb24a21d9954f029",
            "a9a2bd63897d438aa5dab26f3def0a0c",
            "a64db6ab3e06456e949d953bf6c247bf",
            "e858b582663a4c54a9f5eaef6eb4819c",
            "a07a5f64d5c348608c5d2f7466bd6e64",
            "9db6a444e90944058bb20fd55700438a",
            "24836281928f4d0f97575d2d64a9fd95",
            "c703758f073f4ae99d530f956c3db26d",
            "f2534d57a66940bfab23014cd74eeba0",
            "7c1b07ef9c224b338419647304430714",
            "e88316d74be147a4bf439a796f04b264",
            "67d699f381f14353882b65c98fc6c9d7",
            "c81952452c7442febf8b9d73df9f8882",
            "3b052ac3c2a546d5a2a16a74a13ba441",
            "5673fb78143144d89cf3aa3c77abc6e3",
            "2555af15277c4c28b3f26afab6af945e",
            "364850298f5a4ee6b8eae1d34b81ab60",
            "3cd12f17cdb845df80901e31a0c5a267",
            "1c0d8886408e444caefdb9c3570aaa1f",
            "3787de10d67f4f85a78945cdd09207c8",
            "541e28e7695c4f34bb826281086fcc75",
            "2e2137d3f8f84d0ea81040fa1e5180d4",
            "5727f385ef2b4ad0856e76dae6099f9e",
            "4379857df76d4407aa3dd930ab8b261e",
            "315e331616ca4b1ca106e9f67a049e11",
            "22ed1d309c4040fbaee3f535f804d089",
            "ef29e1d0764b413d88746ee8c6d5233a",
            "018636d597304d7195c22460f8558c9b",
            "a306b29e29394f2c964dc7493493fd4c",
            "cf5dc7fca05d4de99c528266fd51801b",
            "68139cac91804b20bf79320e35b6bf6b",
            "e482c95af993411f8beb6638a5e8dd25",
            "44a1c7a6680a45d68037a4543deaba2f",
            "3a39a4f1b93c4458a7481fcc6148e307",
            "0861de5b18674defbb1ed7c340cffaa7",
            "f5da32ddccb744149ab408dba51277eb",
            "707603eaa3c24ccca97e3a0c41713420",
            "41d0ee4ae57a4ad8b7fb299e78cf685a",
            "fcaf2d1b4a2e44598b0f679a57d10407",
            "69a2d93b04524993a0a42683a72d9cbb",
            "8ddf2d2c83bf449c84c1f9ee4e71bbdc",
            "85a7e569640a41ccb6f6284ed35c4bb1",
            "67f5e0daa91c4eceb237469d9fa55ff0",
            "fd3613daf67f481aaf69d569a76ff3a7",
            "a751398870ea4acf8cf98ebc3032b68a",
            "b5f6c4509eaa4f9199b47c3506b3bdf0",
            "4f20c81e7b9b44beba3e0f2e151dd48d",
            "90e8277fdaad4382ae3062243ceb2074",
            "3eacc336ce0b420b88a34e30402829e1",
            "2ba959a471014944a88c976a4d49c7c2",
            "afea99cb9c074cff9e4acf59b31f55a5",
            "6a9fd5ed385a4fb3ad1904299cfdb86f",
            "ead7f6210fb3435c9803802b9fec2a61",
            "2cd0ad04f26842b6907c8cddb4f26728",
            "5da6977b054044fd90c28397e841c8eb",
            "8e1812c4b37449ee941043d47b57e793",
            "ebb88cccf9c74205a9996a52e3b473c7",
            "c4e31d6367a14841ad4be29e854466d4",
            "acfb1ee87b534e4f9a40f9ba65b24166",
            "c2c3941f76514868b869b54c9fc7bf70"
          ]
        },
        "id": "FVjc1hmv4Iwr",
        "outputId": "991234cd-9408-4681-b6ba-52ff1072d881"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U \"transformers>=4.51.0\" \"datasets>=2.18.0\" \"accelerate>=0.33.0\" \"peft>=0.12.0\" \"bitsandbytes>=0.43.0\" \"safetensors>=0.4.3\" \"tqdm>=4.66.0\"\n",
        "\n",
        "import os, re, random, time\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling, set_seed\n",
        "from transformers.trainer_callback import TrainerCallback\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "print(\"cuda:\", torch.cuda.is_available())\n",
        "assert torch.cuda.is_available(), \"GPU не включен: Runtime -> Change runtime type -> GPU\"\n",
        "print(\"gpu:\", torch.cuda.get_device_name(0))\n",
        "!nvidia-smi\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "prefix_path = \"/content/prefixes.txt\"\n",
        "assert os.path.exists(prefix_path), f\"Нет файла {prefix_path}\"\n",
        "\n",
        "def parse_prefixes(path):\n",
        "    items = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            s = line.strip(\"\\n\").strip()\n",
        "            if not s:\n",
        "                continue\n",
        "            m = re.match(r\"^\\s*(\\d+)\\s+(.*)\\s*$\", s)\n",
        "            if m:\n",
        "                idx = int(m.group(1))\n",
        "                pref = m.group(2).strip()\n",
        "                items.append((idx, pref))\n",
        "            else:\n",
        "                items.append((len(items), s))\n",
        "    items.sort(key=lambda x: x[0])\n",
        "    return items\n",
        "\n",
        "prefixes = parse_prefixes(prefix_path)\n",
        "\n",
        "def build_prefix_regex(prefix):\n",
        "    p = prefix.strip()\n",
        "    p = p.replace(\"—\", \"-\").replace(\"–\", \"-\")\n",
        "    esc = re.escape(p)\n",
        "    esc = esc.replace(r\"\\ \", r\"\\s+\")\n",
        "    esc = esc.replace(\"ё\", \"[её]\").replace(\"Ё\", \"[ЕЁ]\")\n",
        "    esc = esc.replace(r\"\\-\", r\"[-—–]\")\n",
        "    return re.compile(r\"^\\s*\" + esc + r\"(?:(?:\\s*[,:\\-—–]\\s*)|\\s+|$)\", re.IGNORECASE)\n",
        "\n",
        "patterns = {idx: build_prefix_regex(pref) for idx, pref in prefixes}\n",
        "\n",
        "def cleanup_one_line(s):\n",
        "    s = s.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = s.strip(\" ,;:-—–\")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def safe_continuation(full, prefix):\n",
        "    full_s = full.strip()\n",
        "    pref_s = prefix.strip()\n",
        "    if full_s.lower().startswith(pref_s.lower()):\n",
        "        cont = full_s[len(pref_s):]\n",
        "    else:\n",
        "        pos = full_s.lower().find(pref_s.lower())\n",
        "        cont = full_s[pos + len(pref_s):] if pos >= 0 else full_s\n",
        "    return cleanup_one_line(cont)\n",
        "\n",
        "def score_candidate(s):\n",
        "    if not s:\n",
        "        return -1e9\n",
        "    if len(s) < 25:\n",
        "        return -200 + len(s)\n",
        "    if len(s) > 280:\n",
        "        return -50 - (len(s) - 280)\n",
        "    words = re.findall(r\"[A-Za-zА-Яа-яЁё0-9]+\", s)\n",
        "    uniq = len(set(w.lower() for w in words)) / max(1, len(words))\n",
        "    rep_pen = 0\n",
        "    if len(words) >= 10:\n",
        "        for i in range(len(words) - 6):\n",
        "            if words[i:i+3] == words[i+3:i+6]:\n",
        "                rep_pen += 1\n",
        "    end_bonus = 10 if re.search(r\"[.!?…]$\", s) else 0\n",
        "    return 60 * uniq + end_bonus - 25 * rep_pen - 0.03 * abs(len(s) - 140)\n",
        "\n",
        "per_prefix_real_limit = 60\n",
        "general_limit = 6000\n",
        "max_scan = 90000\n",
        "\n",
        "counts = {idx: 0 for idx, _ in prefixes}\n",
        "active = set(counts.keys())\n",
        "real_texts = []\n",
        "general_texts = []\n",
        "\n",
        "print(\"Stage: streaming/filtering dataset\")\n",
        "ds_stream = load_dataset(\"igorktech/anekdots\", split=\"train\", streaming=True)\n",
        "\n",
        "pbar = tqdm(total=max_scan, desc=\"scan\", unit=\"ex\")\n",
        "scanned = 0\n",
        "for ex in ds_stream:\n",
        "    scanned += 1\n",
        "    pbar.update(1)\n",
        "    if scanned >= max_scan:\n",
        "        break\n",
        "\n",
        "    t = ex.get(\"text\", None)\n",
        "    if not isinstance(t, str):\n",
        "        continue\n",
        "    t = t.strip()\n",
        "    if len(t) < 60 or len(t) > 900:\n",
        "        continue\n",
        "    if \"\\u0000\" in t:\n",
        "        continue\n",
        "    mark = ex.get(\"total_mark\", None)\n",
        "    if isinstance(mark, int) and mark < 3:\n",
        "        continue\n",
        "\n",
        "    matched = False\n",
        "    if active:\n",
        "        for idx in list(active):\n",
        "            if counts[idx] >= per_prefix_real_limit:\n",
        "                active.discard(idx)\n",
        "                continue\n",
        "            if patterns[idx].match(t):\n",
        "                real_texts.append(t)\n",
        "                counts[idx] += 1\n",
        "                matched = True\n",
        "                if counts[idx] >= per_prefix_real_limit:\n",
        "                    active.discard(idx)\n",
        "                break\n",
        "\n",
        "    if (not matched) and (len(general_texts) < general_limit):\n",
        "        if isinstance(mark, int) and mark >= 10:\n",
        "            general_texts.append(t)\n",
        "\n",
        "    if scanned % 2000 == 0:\n",
        "        pbar.set_postfix({\"real\": len(real_texts), \"gen\": len(general_texts), \"active\": len(active)})\n",
        "\n",
        "    if (len(general_texts) >= general_limit) and (not active):\n",
        "        break\n",
        "\n",
        "pbar.close()\n",
        "print(\"Stage done:\", {\"scanned\": scanned, \"real\": len(real_texts), \"gen\": len(general_texts), \"active\": len(active)})\n",
        "\n",
        "train_texts = real_texts + general_texts\n",
        "random.shuffle(train_texts)\n",
        "\n",
        "adapter_dir = \"/content/qwen3_0p6b_ru_jokes_lora\"\n",
        "os.makedirs(adapter_dir, exist_ok=True)\n",
        "\n",
        "base_model_id = \"Qwen/Qwen3-0.6B-Base\"\n",
        "print(\"Model:\", base_model_id)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True, trust_remote_code=True)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=compute_dtype\n",
        ")\n",
        "\n",
        "def load_base():\n",
        "    return AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=compute_dtype,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "def generate_candidates(m, tok, prefix, n, max_new_tokens=90, temperature=1.0, top_p=0.9, rep_pen=1.12):\n",
        "    inp = tok(prefix, return_tensors=\"pt\")\n",
        "    input_ids = inp[\"input_ids\"].to(m.device)\n",
        "    attn = inp[\"attention_mask\"].to(m.device)\n",
        "    input_ids = input_ids.repeat(n, 1)\n",
        "    attn = attn.repeat(n, 1)\n",
        "    with torch.no_grad():\n",
        "        out = m.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            repetition_penalty=rep_pen,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tok.eos_token_id,\n",
        "            eos_token_id=tok.eos_token_id\n",
        "        )\n",
        "    return [tok.decode(out[i], skip_special_tokens=True) for i in range(out.size(0))]\n",
        "\n",
        "adapter_exists = os.path.exists(os.path.join(adapter_dir, \"adapter_config.json\"))\n",
        "\n",
        "if not adapter_exists:\n",
        "    model_for_aug = load_base()\n",
        "    model_for_aug.eval()\n",
        "    aug_per_prefix = 1\n",
        "    aug_texts = []\n",
        "    for _, pref in tqdm(prefixes, desc=\"augment\", unit=\"pref\"):\n",
        "        fulls = generate_candidates(model_for_aug, tokenizer, pref, n=aug_per_prefix, max_new_tokens=80, temperature=1.02, top_p=0.9, rep_pen=1.10)\n",
        "        for ft in fulls:\n",
        "            ft = cleanup_one_line(ft)\n",
        "            if len(ft) >= len(pref) + 20:\n",
        "                aug_texts.append(ft)\n",
        "    train_texts = train_texts + aug_texts\n",
        "    random.shuffle(train_texts)\n",
        "\n",
        "    model = load_base()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "    train_ds = Dataset.from_dict({\"text\": train_texts})\n",
        "\n",
        "    max_len = 192\n",
        "    def tok_fn(batch):\n",
        "        return tokenizer(batch[\"text\"], truncation=True, max_length=max_len, padding=False)\n",
        "\n",
        "    tok_ds = train_ds.map(tok_fn, batched=True, remove_columns=[\"text\"], desc=\"tokenize\")\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    class TrainTimeCallback(TrainerCallback):\n",
        "        def __init__(self):\n",
        "            self.t0 = None\n",
        "        def on_train_begin(self, args, state, control, **kwargs):\n",
        "            self.t0 = time.time()\n",
        "            print(f\"train_begin: max_steps={state.max_steps} epochs={args.num_train_epochs} bs={args.per_device_train_batch_size} ga={args.gradient_accumulation_steps} bf16={args.bf16} fp16={args.fp16}\")\n",
        "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "            if logs is None:\n",
        "                return\n",
        "            now = time.time()\n",
        "            elapsed = now - (self.t0 or now)\n",
        "            step = state.global_step\n",
        "            max_steps = state.max_steps or 0\n",
        "            eta = None\n",
        "            if max_steps and step:\n",
        "                eta = elapsed * (max_steps - step) / max(1, step)\n",
        "            loss = logs.get(\"loss\", None)\n",
        "            lr = logs.get(\"learning_rate\", None)\n",
        "            ep = state.epoch\n",
        "            s = f\"step={step}/{max_steps} epoch={ep:.3f}\" if ep is not None else f\"step={step}/{max_steps}\"\n",
        "            if loss is not None:\n",
        "                s += f\" loss={loss:.4f}\"\n",
        "            if lr is not None:\n",
        "                s += f\" lr={lr:.2e}\"\n",
        "            s += f\" elapsed={elapsed/60:.1f}m\"\n",
        "            if eta is not None:\n",
        "                s += f\" eta={eta/60:.1f}m\"\n",
        "            print(s)\n",
        "        def on_train_end(self, args, state, control, **kwargs):\n",
        "            if self.t0 is None:\n",
        "                return\n",
        "            elapsed = time.time() - self.t0\n",
        "            print(f\"train_end: steps={state.global_step} time={elapsed/60:.1f}m\")\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=adapter_dir,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        learning_rate=2e-4,\n",
        "        warmup_ratio=0.03,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"no\",\n",
        "        bf16=use_bf16,\n",
        "        fp16=not use_bf16,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to=\"none\",\n",
        "        dataloader_num_workers=2,\n",
        "        dataloader_pin_memory=True,\n",
        "        group_by_length=True,\n",
        "        disable_tqdm=False\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=tok_ds,\n",
        "        data_collator=data_collator,\n",
        "        callbacks=[TrainTimeCallback()]\n",
        "    )\n",
        "\n",
        "    print(\"Stage: training\")\n",
        "    trainer.train()\n",
        "    model.save_pretrained(adapter_dir)\n",
        "    tokenizer.save_pretrained(adapter_dir)\n",
        "\n",
        "base_model = load_base()\n",
        "model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
        "model.eval()\n",
        "\n",
        "def generate_batch(prefix, n, max_new_tokens=110, temperature=1.03, top_p=0.9, rep_pen=1.12):\n",
        "    inp = tokenizer(prefix, return_tensors=\"pt\")\n",
        "    input_ids = inp[\"input_ids\"].to(model.device)\n",
        "    attn = inp[\"attention_mask\"].to(model.device)\n",
        "    input_ids = input_ids.repeat(n, 1)\n",
        "    attn = attn.repeat(n, 1)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attn,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            repetition_penalty=rep_pen,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    return [tokenizer.decode(out[i], skip_special_tokens=True) for i in range(out.size(0))]\n",
        "\n",
        "submission_path = \"/content/submission.txt\"\n",
        "num_lines_per_prefix = 3\n",
        "candidates_per_prefix = 12\n",
        "\n",
        "print(\"Stage: generating submission\")\n",
        "with open(submission_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for idx, pref in tqdm(prefixes, desc=\"prefixes\", unit=\"pref\"):\n",
        "        fulls = generate_batch(pref, candidates_per_prefix)\n",
        "        cands = []\n",
        "        for ft in fulls:\n",
        "            cont = safe_continuation(ft, pref)\n",
        "            if cont:\n",
        "                cands.append(cont)\n",
        "        cands = list(dict.fromkeys(cands))\n",
        "        cands.sort(key=score_candidate, reverse=True)\n",
        "        picked = cands[:num_lines_per_prefix] if cands else [\"...\"]\n",
        "        for cont in picked:\n",
        "            f.write(f\"{idx} {cont}\\n\")\n",
        "\n",
        "print(\"saved:\", submission_path)\n",
        "with open(submission_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(15):\n",
        "        line = f.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        print(line.rstrip(\"\\n\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb41963",
      "metadata": {},
      "source": [
        "вырезал с префиксом 0"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    },
    "widgets": {
      "state": {},
      "version_major": 2,
      "version_minor": 0
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
