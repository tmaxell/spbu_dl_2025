{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d9f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self):\n",
    "        for param in self._parameters.values():\n",
    "            yield param\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632246b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = torch.ones(num_features, requires_grad=True)\n",
    "        self.beta = torch.zeros(num_features, requires_grad=True)\n",
    "        self._parameters['gamma'] = self.gamma\n",
    "        self._parameters['beta'] = self.beta\n",
    "\n",
    "        self.running_mean = torch.zeros(num_features)\n",
    "        self.running_var = torch.ones(num_features)\n",
    "        \n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        dims = [0]\n",
    "        if x.dim() == 4:\n",
    "            dims.extend([2, 3])\n",
    "        \n",
    "        shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "        gamma = self.gamma.view(*shape)\n",
    "        beta = self.beta.view(*shape)\n",
    "\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(dim=dims, keepdim=True)\n",
    "            batch_var_biased = x.var(dim=dims, unbiased=False, keepdim=True)\n",
    "\n",
    "            x_hat = (x - batch_mean) / torch.sqrt(batch_var_biased + self.eps)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.squeeze()\n",
    "                \n",
    "                n = x.numel() / x.shape[1]\n",
    "                batch_var_unbiased = batch_var_biased * (n / (n - 1))\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var_unbiased.squeeze()\n",
    "\n",
    "            self.cache = {\n",
    "                'x': x,\n",
    "                'x_hat': x_hat,\n",
    "                'gamma': gamma,\n",
    "                'batch_mean': batch_mean,\n",
    "                'batch_var': batch_var_biased,\n",
    "                'dims': dims\n",
    "            }\n",
    "        else:\n",
    "            mean = self.running_mean.view(*shape)\n",
    "            var = self.running_var.view(*shape)\n",
    "            x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        out = gamma * x_hat + beta\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x, x_hat, gamma, batch_mean, batch_var, dims = \\\n",
    "            self.cache['x'], self.cache['x_hat'], self.cache['gamma'], \\\n",
    "            self.cache['batch_mean'], self.cache['batch_var'], self.cache['dims']\n",
    "\n",
    "        N = x.shape[0]\n",
    "        if x.dim() == 4:\n",
    "            N *= x.shape[2] * x.shape[3]\n",
    "\n",
    "        self.beta.grad = grad_output.sum(dim=dims)\n",
    "        self.gamma.grad = (grad_output * x_hat).sum(dim=dims)\n",
    "        \n",
    "        grad_x_hat = grad_output * gamma\n",
    "        \n",
    "        inv_std = 1. / torch.sqrt(batch_var + self.eps)\n",
    "        grad_var = -0.5 * (grad_x_hat * (x - batch_mean) * (inv_std**3)).sum(dim=dims, keepdim=True)\n",
    "        \n",
    "        grad_mean = -1. * (grad_x_hat * inv_std).sum(dim=dims, keepdim=True) + \\\n",
    "                    grad_var * (-2./N) * (x - batch_mean).sum(dim=dims, keepdim=True)\n",
    "\n",
    "        grad_x = grad_x_hat * inv_std + \\\n",
    "                 grad_var * (2./N) * (x - batch_mean) + \\\n",
    "                 grad_mean / N\n",
    "                 \n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77db9160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Проверка в режиме TRAIN ---\n",
      "Выходы forward совпадают: True\n",
      "running_mean совпадают: True\n",
      "running_var совпадают: True\n",
      "Градиенты по входу dX совпадают: True\n",
      "Градиенты по gamma/weight совпадают: True\n",
      "Градиенты по beta/bias совпадают: True\n",
      "--------------------\n",
      "\n",
      "--- Проверка в режиме EVAL ---\n",
      "Выходы forward в режиме eval совпадают: True\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "N, C = 8, 3\n",
    "num_features = C\n",
    "eps = 1e-5\n",
    "momentum = 0.1\n",
    "\n",
    "my_bn = BatchNorm(num_features, eps=eps, momentum=momentum)\n",
    "torch_bn = torch.nn.BatchNorm1d(num_features, eps=eps, momentum=momentum)\n",
    "\n",
    "with torch.no_grad():\n",
    "    my_bn.gamma.copy_(torch_bn.weight)\n",
    "    my_bn.beta.copy_(torch_bn.bias)\n",
    "    my_bn.running_mean.copy_(torch_bn.running_mean)\n",
    "    my_bn.running_var.copy_(torch_bn.running_var)\n",
    "    \n",
    "x = torch.randn(N, C)\n",
    "x.requires_grad_(True)\n",
    "my_x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "print(\"--- Проверка в режиме TRAIN ---\")\n",
    "\n",
    "my_bn.train()\n",
    "torch_bn.train()\n",
    "my_out = my_bn(my_x)\n",
    "torch_out = torch_bn(x)\n",
    "\n",
    "print(f\"Выходы forward совпадают: {torch.allclose(my_out, torch_out)}\")\n",
    "print(f\"running_mean совпадают: {torch.allclose(my_bn.running_mean, torch_bn.running_mean)}\")\n",
    "print(f\"running_var совпадают: {torch.allclose(my_bn.running_var, torch_bn.running_var)}\")\n",
    "\n",
    "grad_output = torch.randn_like(x)\n",
    "torch_out.backward(grad_output)\n",
    "my_grad_x = my_bn.backward(grad_output.clone())\n",
    "\n",
    "print(f\"Градиенты по входу dX совпадают: {torch.allclose(my_grad_x, x.grad)}\")\n",
    "print(f\"Градиенты по gamma/weight совпадают: {torch.allclose(my_bn.gamma.grad, torch_bn.weight.grad)}\")\n",
    "print(f\"Градиенты по beta/bias совпадают: {torch.allclose(my_bn.beta.grad, torch_bn.bias.grad)}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(\"\\n--- Проверка в режиме EVAL ---\")\n",
    "\n",
    "my_bn.eval()\n",
    "torch_bn.eval()\n",
    "\n",
    "x_eval = torch.randn(N, C)\n",
    "my_out_eval = my_bn(x_eval)\n",
    "torch_out_eval = torch_bn(x_eval)\n",
    "\n",
    "print(f\"Выходы forward в режиме eval совпадают: {torch.allclose(my_out_eval, torch_out_eval)}\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933540cf",
   "metadata": {},
   "source": [
    "Реализация linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n",
    "        self.W = torch.empty(out_features, in_features).uniform_(-bound, bound)\n",
    "        self.b = torch.empty(out_features).uniform_(-bound, bound)\n",
    "        \n",
    "        self.W.requires_grad = True\n",
    "        self.b.requires_grad = True\n",
    "        \n",
    "        self._parameters['W'] = self.W\n",
    "        self._parameters['b'] = self.b\n",
    "        \n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache['x'] = x\n",
    "        output = x @ self.W.T + self.b\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x = self.cache['x']\n",
    "        \n",
    "        self.W.grad = grad_output.T @ x\n",
    "        self.b.grad = grad_output.sum(dim=0)\n",
    "        \n",
    "        grad_x = grad_output @ self.W\n",
    "        \n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, D_out = 16, 100, 10\n",
    "\n",
    "my_linear = Linear(D_in, D_out)\n",
    "torch_linear = torch.nn.Linear(D_in, D_out)\n",
    "\n",
    "with torch.no_grad():\n",
    "    my_linear.W.copy_(torch_linear.weight)\n",
    "    my_linear.b.copy_(torch_linear.bias)\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "x.requires_grad_(True)\n",
    "my_x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "my_out = my_linear(my_x)\n",
    "torch_out = torch_linear(x)\n",
    "\n",
    "grad_output = torch.randn(N, D_out)\n",
    "torch_out.backward(grad_output)\n",
    "my_grad_x = my_linear.backward(grad_output.clone())\n",
    "\n",
    "print(\"--- Проверка слоя Linear ---\")\n",
    "print(f\"Выходы forward совпадают: {torch.allclose(my_out, torch_out)}\")\n",
    "print(f\"Градиенты по входу dX совпадают: {torch.allclose(my_grad_x, x.grad)}\")\n",
    "print(f\"Градиенты по весам dW совпадают: {torch.allclose(my_linear.W.grad, torch_linear.weight.grad)}\")\n",
    "print(f\"Градиенты по смещению db совпадают: {torch.allclose(my_linear.b.grad, torch_linear.bias.grad)}\")\n",
    "print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
