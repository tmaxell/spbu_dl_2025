{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29d9f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        self._parameters = {}\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self):\n",
    "        for param in self._parameters.values():\n",
    "            yield param\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "632246b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = torch.ones(num_features, requires_grad=True)\n",
    "        self.beta = torch.zeros(num_features, requires_grad=True)\n",
    "        self._parameters['gamma'] = self.gamma\n",
    "        self._parameters['beta'] = self.beta\n",
    "\n",
    "        self.running_mean = torch.zeros(num_features)\n",
    "        self.running_var = torch.ones(num_features)\n",
    "        \n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        dims = [0]\n",
    "        if x.dim() == 4:\n",
    "            dims.extend([2, 3])\n",
    "        \n",
    "        shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "        gamma = self.gamma.view(*shape)\n",
    "        beta = self.beta.view(*shape)\n",
    "\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(dim=dims, keepdim=True)\n",
    "            batch_var_biased = x.var(dim=dims, unbiased=False, keepdim=True)\n",
    "\n",
    "            x_hat = (x - batch_mean) / torch.sqrt(batch_var_biased + self.eps)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.squeeze()\n",
    "                \n",
    "                n = x.numel() / x.shape[1]\n",
    "                batch_var_unbiased = batch_var_biased * (n / (n - 1))\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var_unbiased.squeeze()\n",
    "\n",
    "            self.cache = {\n",
    "                'x': x,\n",
    "                'x_hat': x_hat,\n",
    "                'gamma': gamma,\n",
    "                'batch_mean': batch_mean,\n",
    "                'batch_var': batch_var_biased,\n",
    "                'dims': dims\n",
    "            }\n",
    "        else:\n",
    "            mean = self.running_mean.view(*shape)\n",
    "            var = self.running_var.view(*shape)\n",
    "            x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "\n",
    "        out = gamma * x_hat + beta\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x, x_hat, gamma, batch_mean, batch_var, dims = \\\n",
    "            self.cache['x'], self.cache['x_hat'], self.cache['gamma'], \\\n",
    "            self.cache['batch_mean'], self.cache['batch_var'], self.cache['dims']\n",
    "\n",
    "        N = x.shape[0]\n",
    "        if x.dim() == 4:\n",
    "            N *= x.shape[2] * x.shape[3]\n",
    "\n",
    "        self.beta.grad = grad_output.sum(dim=dims)\n",
    "        self.gamma.grad = (grad_output * x_hat).sum(dim=dims)\n",
    "        \n",
    "        grad_x_hat = grad_output * gamma\n",
    "        \n",
    "        inv_std = 1. / torch.sqrt(batch_var + self.eps)\n",
    "        grad_var = -0.5 * (grad_x_hat * (x - batch_mean) * (inv_std**3)).sum(dim=dims, keepdim=True)\n",
    "        \n",
    "        grad_mean = -1. * (grad_x_hat * inv_std).sum(dim=dims, keepdim=True) + \\\n",
    "                    grad_var * (-2./N) * (x - batch_mean).sum(dim=dims, keepdim=True)\n",
    "\n",
    "        grad_x = grad_x_hat * inv_std + \\\n",
    "                 grad_var * (2./N) * (x - batch_mean) + \\\n",
    "                 grad_mean / N\n",
    "                 \n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77db9160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Проверка в режиме TRAIN ---\n",
      "Выходы forward совпадают: True\n",
      "running_mean совпадают: True\n",
      "running_var совпадают: True\n",
      "Градиенты по входу dX совпадают: True\n",
      "Градиенты по gamma/weight совпадают: True\n",
      "Градиенты по beta/bias совпадают: True\n",
      "--------------------\n",
      "\n",
      "--- Проверка в режиме EVAL ---\n",
      "Выходы forward в режиме eval совпадают: True\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "N, C = 8, 3\n",
    "num_features = C\n",
    "eps = 1e-5\n",
    "momentum = 0.1\n",
    "\n",
    "my_bn = BatchNorm(num_features, eps=eps, momentum=momentum)\n",
    "torch_bn = torch.nn.BatchNorm1d(num_features, eps=eps, momentum=momentum)\n",
    "\n",
    "with torch.no_grad():\n",
    "    my_bn.gamma.copy_(torch_bn.weight)\n",
    "    my_bn.beta.copy_(torch_bn.bias)\n",
    "    my_bn.running_mean.copy_(torch_bn.running_mean)\n",
    "    my_bn.running_var.copy_(torch_bn.running_var)\n",
    "    \n",
    "x = torch.randn(N, C)\n",
    "x.requires_grad_(True)\n",
    "my_x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "print(\"--- Проверка в режиме TRAIN ---\")\n",
    "\n",
    "my_bn.train()\n",
    "torch_bn.train()\n",
    "my_out = my_bn(my_x)\n",
    "torch_out = torch_bn(x)\n",
    "\n",
    "print(f\"Выходы forward совпадают: {torch.allclose(my_out, torch_out)}\")\n",
    "print(f\"running_mean совпадают: {torch.allclose(my_bn.running_mean, torch_bn.running_mean)}\")\n",
    "print(f\"running_var совпадают: {torch.allclose(my_bn.running_var, torch_bn.running_var)}\")\n",
    "\n",
    "grad_output = torch.randn_like(x)\n",
    "torch_out.backward(grad_output)\n",
    "my_grad_x = my_bn.backward(grad_output.clone())\n",
    "\n",
    "print(f\"Градиенты по входу dX совпадают: {torch.allclose(my_grad_x, x.grad)}\")\n",
    "print(f\"Градиенты по gamma/weight совпадают: {torch.allclose(my_bn.gamma.grad, torch_bn.weight.grad)}\")\n",
    "print(f\"Градиенты по beta/bias совпадают: {torch.allclose(my_bn.beta.grad, torch_bn.bias.grad)}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "print(\"\\n--- Проверка в режиме EVAL ---\")\n",
    "\n",
    "my_bn.eval()\n",
    "torch_bn.eval()\n",
    "\n",
    "x_eval = torch.randn(N, C)\n",
    "my_out_eval = my_bn(x_eval)\n",
    "torch_out_eval = torch_bn(x_eval)\n",
    "\n",
    "print(f\"Выходы forward в режиме eval совпадают: {torch.allclose(my_out_eval, torch_out_eval)}\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933540cf",
   "metadata": {},
   "source": [
    "Реализация linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1d1c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        bound = 1 / math.sqrt(in_features) if in_features > 0 else 0\n",
    "        self.W = torch.empty(out_features, in_features).uniform_(-bound, bound)\n",
    "        self.b = torch.empty(out_features).uniform_(-bound, bound)\n",
    "        \n",
    "        self.W.requires_grad = True\n",
    "        self.b.requires_grad = True\n",
    "        \n",
    "        self._parameters['W'] = self.W\n",
    "        self._parameters['b'] = self.b\n",
    "        \n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache['x'] = x\n",
    "        output = x @ self.W.T + self.b\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x = self.cache['x']\n",
    "        \n",
    "        self.W.grad = grad_output.T @ x\n",
    "        self.b.grad = grad_output.sum(dim=0)\n",
    "        \n",
    "        grad_x = grad_output @ self.W\n",
    "        \n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31af5f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Проверка слоя Linear ---\n",
      "Выходы forward совпадают: True\n",
      "Градиенты по входу dX совпадают: True\n",
      "Градиенты по весам dW совпадают: True\n",
      "Градиенты по смещению db совпадают: True\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "N, D_in, D_out = 16, 100, 10\n",
    "\n",
    "my_linear = Linear(D_in, D_out)\n",
    "torch_linear = torch.nn.Linear(D_in, D_out)\n",
    "\n",
    "with torch.no_grad():\n",
    "    my_linear.W.copy_(torch_linear.weight)\n",
    "    my_linear.b.copy_(torch_linear.bias)\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "x.requires_grad_(True)\n",
    "my_x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "my_out = my_linear(my_x)\n",
    "torch_out = torch_linear(x)\n",
    "\n",
    "grad_output = torch.randn(N, D_out)\n",
    "torch_out.backward(grad_output)\n",
    "my_grad_x = my_linear.backward(grad_output.clone())\n",
    "\n",
    "print(\"--- Проверка слоя Linear ---\")\n",
    "print(f\"Выходы forward совпадают: {torch.allclose(my_out, torch_out)}\")\n",
    "print(f\"Градиенты по входу dX совпадают: {torch.allclose(my_grad_x, x.grad)}\")\n",
    "print(f\"Градиенты по весам dW совпадают: {torch.allclose(my_linear.W.grad, torch_linear.weight.grad)}\")\n",
    "print(f\"Градиенты по смещению db совпадают: {torch.allclose(my_linear.b.grad, torch_linear.bias.grad)}\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46dfaab",
   "metadata": {},
   "source": [
    "Реализация dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a120cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        if p < 0 or p >= 1:\n",
    "            raise ValueError(\"dropout probability has to be between 0 and 1, but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.scale = 1.0 / (1.0 - self.p)\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0:\n",
    "            return x\n",
    "        \n",
    "        mask = (torch.rand_like(x) > self.p).float() * self.scale\n",
    "        self.cache['mask'] = mask\n",
    "        \n",
    "        return x * mask\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if not self.training or self.p == 0:\n",
    "            return grad_output\n",
    "            \n",
    "        mask = self.cache.get('mask')\n",
    "        if mask is None:\n",
    "            raise RuntimeError(\"backward called without a prior forward call in train mode\")\n",
    "            \n",
    "        return grad_output * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a4bfb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Проверка в режиме TRAIN ---\n",
      "Вероятность зануления (p): 0.5\n",
      "Количество элементов в тензоре: 100000\n",
      "Фактическое количество нулей: 49980\n",
      "Ожидаемое количество нулей (примерно): 50000\n",
      "Значение не-нулевых элементов: 2.0\n",
      "Ожидаемое значение (1 / (1 - p)): 2.0\n",
      "Статистика в режиме train выглядит корректно: True\n",
      "--------------------\n",
      "\n",
      "--- Проверка в режиме EVAL ---\n",
      "Выход в режиме eval идентичен входу: True\n",
      "--------------------\n",
      "\n",
      "--- Проверка backward ---\n",
      "Градиент на входе равен grad_output * mask: True\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "p = 0.5\n",
    "N, D = 1000, 100\n",
    "\n",
    "my_dropout = Dropout(p=p)\n",
    "x = torch.ones(N, D)\n",
    "\n",
    "# --- Проверка в режиме TRAIN ---\n",
    "print(\"--- Проверка в режиме TRAIN ---\")\n",
    "my_dropout.train()\n",
    "out_train = my_dropout(x)\n",
    "\n",
    "num_zeros = (out_train == 0).sum().item()\n",
    "expected_zeros = p * x.numel()\n",
    "scale = 1.0 / (1.0 - p)\n",
    "non_zero_val = out_train[out_train != 0][0].item()\n",
    "\n",
    "print(f\"Вероятность зануления (p): {p}\")\n",
    "print(f\"Количество элементов в тензоре: {x.numel()}\")\n",
    "print(f\"Фактическое количество нулей: {num_zeros}\")\n",
    "print(f\"Ожидаемое количество нулей (примерно): {expected_zeros:.0f}\")\n",
    "print(f\"Значение не-нулевых элементов: {non_zero_val:.1f}\")\n",
    "print(f\"Ожидаемое значение (1 / (1 - p)): {scale:.1f}\")\n",
    "print(f\"Статистика в режиме train выглядит корректно: {abs(num_zeros - expected_zeros) < 100 and torch.allclose(torch.tensor(non_zero_val), torch.tensor(scale))}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# --- Проверка в режиме EVAL ---\n",
    "print(\"\\n--- Проверка в режиме EVAL ---\")\n",
    "my_dropout.eval()\n",
    "out_eval = my_dropout(x)\n",
    "\n",
    "print(f\"Выход в режиме eval идентичен входу: {torch.allclose(out_eval, x)}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# --- Проверка backward ---\n",
    "print(\"\\n--- Проверка backward ---\")\n",
    "my_dropout.train()\n",
    "x_rand = torch.randn(N, D)\n",
    "out = my_dropout(x_rand)\n",
    "grad_out = torch.randn_like(x_rand)\n",
    "grad_in = my_dropout.backward(grad_out)\n",
    "mask = my_dropout.cache['mask']\n",
    "\n",
    "print(f\"Градиент на входе равен grad_output * mask: {torch.allclose(grad_in, grad_out * mask)}\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426614a",
   "metadata": {},
   "source": [
    "Реализация sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d49936b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = 1 / (1 + torch.exp(-x))\n",
    "        self.cache['output'] = output\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        output = self.cache['output']\n",
    "        local_grad = output * (1 - output)\n",
    "        return grad_output * local_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a23ec1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Проверка слоя Sigmoid ---\n",
      "Выходы forward совпадают: True\n",
      "Градиенты по входу dX совпадают: True\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "N, D = 10, 5\n",
    "\n",
    "my_sigmoid = Sigmoid()\n",
    "torch_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "x = torch.randn(N, D)\n",
    "x.requires_grad_(True)\n",
    "my_x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "my_out = my_sigmoid(my_x)\n",
    "torch_out = torch_sigmoid(x)\n",
    "\n",
    "grad_output = torch.randn(N, D)\n",
    "torch_out.backward(grad_output)\n",
    "my_grad_x = my_sigmoid.backward(grad_output.clone())\n",
    "\n",
    "print(\"--- Проверка слоя Sigmoid ---\")\n",
    "print(f\"Выходы forward совпадают: {torch.allclose(my_out, torch_out)}\")\n",
    "print(f\"Градиенты по входу dX совпадают: {torch.allclose(my_grad_x, x.grad)}\")\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5222c68",
   "metadata": {},
   "source": [
    "Реализация ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "388a6419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cache['x'] = x\n",
    "        return torch.maximum(torch.tensor(0.0), x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x = self.cache['x']\n",
    "        mask = (x > 0).float()\n",
    "        return grad_output * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a33357d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Проверка слоя ReLU ---\n",
      "Выходы forward совпадают: True\n",
      "Градиенты по входу dX совпадают: True\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "N, D = 10, 5\n",
    "\n",
    "my_relu = ReLU()\n",
    "torch_relu = torch.nn.ReLU()\n",
    "\n",
    "x = torch.randn(N, D)\n",
    "x.requires_grad_(True)\n",
    "my_x = x.clone().detach().requires_grad_(True)\n",
    "\n",
    "my_out = my_relu(my_x)\n",
    "torch_out = torch_relu(x)\n",
    "\n",
    "grad_output = torch.randn(N, D)\n",
    "torch_out.backward(grad_output)\n",
    "my_grad_x = my_relu.backward(grad_output.clone())\n",
    "\n",
    "print(\"--- Проверка слоя ReLU ---\")\n",
    "print(f\"Выходы forward совпадают: {torch.allclose(my_out, torch_out)}\")\n",
    "print(f\"Градиенты по входу dX совпадают: {torch.allclose(my_grad_x, x.grad)}\")\n",
    "print(\"-\" * 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
